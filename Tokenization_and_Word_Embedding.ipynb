{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization and Word Embedding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdSXaPREGZJ0S47i5kRsL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mind-matrix/research-8th-sem-project/blob/main/Tokenization_and_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7jLY-oFEV-q",
        "outputId": "fe1afd57-c7d1-4c58-a12a-fe36cf8843a9"
      },
      "source": [
        "!pip install --quiet bert_embedding"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 13.8MB 325kB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 57.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 29.6MB 145kB/s \n",
            "\u001b[?25h  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2021.3.4 has requirement numpy>=1.15.1, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 1.7.2 has requirement numpy>=1.16, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.51.2 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEZmF5q_6JKm"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings(action = 'ignore') # Fix for unnecessary warnings in output\r\n",
        "\r\n",
        "import gensim\r\n",
        "from gensim.models import Word2Vec\r\n",
        "import numpy as np\r\n",
        "from scipy import spatial\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "from bert_embedding import BertEmbedding\r\n",
        "import nltk"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Y1pWvZEvGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56a1ce9-9895-4694-8256-2313a38114ca"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eBF3YnmC4_a"
      },
      "source": [
        "text = \"Wikipedia is a free, multilingual open-collaborative online encyclopedia created and maintained by a community of volunteer contributors using a wiki-based editing system. Wikipedia is the largest general reference work on the Internet, and one of the 15 most popular websites as ranked by Alexa; in 2021, it was ranked as the 13th most visited.\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RIYGW8h8NxL"
      },
      "source": [
        "class Word2VecModel:\r\n",
        "  def __init__(self, model):\r\n",
        "    self.model = model\r\n",
        "  def most_similar(self, word, top_n=1):\r\n",
        "    return self.model.most_similar(positive=[word], topn=top_n)\r\n",
        "  def wv(self, word):\r\n",
        "    word = word.lower()\r\n",
        "    try:\r\n",
        "      return self.model.wv[word]\r\n",
        "    except:\r\n",
        "      return self.model.wv[self.most_similar(word)]\r\n",
        "  def embeddings(self, sentences):\r\n",
        "    result = []\r\n",
        "    for i, sentence in enumerate(sent_tokenize(sentences)):\r\n",
        "      result.append([])\r\n",
        "      for word in word_tokenize(sentence):\r\n",
        "        result[i].append(np.array(self.wv(word)))\r\n",
        "      result[i] = np.array(result[i])\r\n",
        "    return np.array(result)\r\n",
        "\r\n",
        "def word2vecModel(text, cbow=True, min_count=1, size=100, window=5, sg=1):\r\n",
        "  sentences = sent_tokenize(text)\r\n",
        "  corpus = []\r\n",
        "  for sentence in sentences:\r\n",
        "    words = [ i.lower() for i in word_tokenize(sentence) ]\r\n",
        "    corpus.append(words)\r\n",
        "  if cbow:\r\n",
        "    model = gensim.models.Word2Vec(corpus, min_count = min_count, size = size, window = window)\r\n",
        "  else:\r\n",
        "    model = gensim.models.Word2Vec(corpus, min_count = min_count, size = size, window = window)\r\n",
        "  return Word2VecModel(model)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ViHZyYlCz1M",
        "outputId": "c863e5d0-a2be-41d0-a899-e3dc3351360f"
      },
      "source": [
        "word2vec_cbow_model = word2vecModel(text, cbow=True)\r\n",
        "print('Word2Vec CBOW Embeddings Shape:')\r\n",
        "embeddings = word2vec_cbow_model.embeddings(text)\r\n",
        "for sent in embeddings:\r\n",
        "  print(sent.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec CBOW Embeddings Shape:\n",
            "(24, 100)\n",
            "(36, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-WRiha0DgOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8be76eb-094a-4f41-b419-d195802178e7"
      },
      "source": [
        "word2vec_sg_model = word2vecModel(text, cbow=False)\r\n",
        "print('Word2Vec SkipGram Embeddings Shape:')\r\n",
        "embeddings = word2vec_sg_model.embeddings(text)\r\n",
        "for sent in embeddings:\r\n",
        "  print(sent.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec SkipGram Embeddings Shape:\n",
            "(24, 100)\n",
            "(36, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRJF9Ql39iTE"
      },
      "source": [
        "class GloveModel:\r\n",
        "  def __init__(self, embeddings_dict):\r\n",
        "    self.embeddings_dict = embeddings_dict\r\n",
        "  def most_similar(self, word, top_n=1):\r\n",
        "    return sorted(self.embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(self.embeddings_dict[word], embedding))[:top_n]\r\n",
        "  def wv(self, word):\r\n",
        "    word = word.lower()\r\n",
        "    return self.embeddings_dict[word] if word in self.embeddings_dict else self.embeddings_dict[self.most_similar(word)]\r\n",
        "  def embeddings(self, sentences):\r\n",
        "    result = []\r\n",
        "    for i, sentence in enumerate(sent_tokenize(sentences)):\r\n",
        "      result.append([])\r\n",
        "      for word in word_tokenize(sentence):\r\n",
        "        result[i].append(np.array(self.wv(word)))\r\n",
        "      result[i] = np.array(result[i])\r\n",
        "    return np.array(result)\r\n",
        "\r\n",
        "def gloveModel(pretrained_embeddings_txt_file):\r\n",
        "  embeddings_dict = dict()\r\n",
        "  with open(pretrained_embeddings_txt_file, 'r', encoding=\"utf-8\") as f:\r\n",
        "    for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0].lower()\r\n",
        "      vector = np.asarray(values[1:], \"float32\")\r\n",
        "      embeddings_dict[word] = vector\r\n",
        "  return GloveModel(embeddings_dict)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sptJq_AbCzLF"
      },
      "source": [
        "# glove_model = gloveModel()\r\n",
        "# print(\"GloVe Embeddings Shape: \")\r\n",
        "# embeddings = glove_model.embeddings(text)\r\n",
        "# for sent in embeddings:\r\n",
        "#   print(sent.shape)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDvDTOeHBB14"
      },
      "source": [
        "class BertModel:\r\n",
        "  def __init__(self, bert):\r\n",
        "    self.bert = bert\r\n",
        "  def embeddings(self, sentences):\r\n",
        "    embeddings = self.bert(sent_tokenize(sentences))\r\n",
        "    return list(map(lambda x: np.array(x[1]), embeddings))\r\n",
        "\r\n",
        "def bertModel():\r\n",
        "  bert = BertEmbedding()\r\n",
        "  return BertModel(bert)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGI4clrSCx3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a803bb8-8877-451e-9a85-233c705a9108"
      },
      "source": [
        "bert_model = bertModel()\r\n",
        "print(\"BERT Embeddings Shape: \")\r\n",
        "embeddings = bert_model.embeddings(text)\r\n",
        "for sent in embeddings:\r\n",
        "  print(sent.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
            "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n",
            "BERT Embeddings Shape: \n",
            "(21, 768)\n",
            "(23, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}