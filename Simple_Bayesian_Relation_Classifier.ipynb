{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Bayesian Relation Classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiO09+JVpLN7COC2AHbsGX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mind-matrix/research-8th-sem-project/blob/main/Simple_Bayesian_Relation_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4hb6GP-3Til"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from operator import mul\r\n",
        "import json\r\n",
        "from tqdm import tqdm\r\n",
        "from functools import reduce\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8T9yZu3cDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0b10ff-3e66-4001-9bb7-dd35b0d6332e"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97zwjoKC3eBT"
      },
      "source": [
        "def train(df, save_to=\"./probabilities.json\"):\r\n",
        "    P_k_count = df['target'].value_counts().to_dict()\r\n",
        "    P_k_N = sum(P_k_count.values())\r\n",
        "    P_k = dict(map(lambda x: (x[0], x[1]/P_k_N), P_k_count.items()))\r\n",
        "\r\n",
        "    P_w_k = dict()\r\n",
        "\r\n",
        "    P_w_nk = dict()\r\n",
        "\r\n",
        "    P_w_count = dict()\r\n",
        "\r\n",
        "    with tqdm(total=len(df.index)) as pbar:\r\n",
        "        for i, row in df.iterrows():\r\n",
        "            pbar.update(1)\r\n",
        "            context = row[\"context\"].replace(row[\"h\"], \"\").replace(row[\"t\"], \"\")\r\n",
        "            words = word_tokenize(context)\r\n",
        "            tokens = [ w.lower() for w in words if w.lower() not in stop_words ]\r\n",
        "            target = row[\"target\"]\r\n",
        "            for token in tokens:\r\n",
        "                if token in P_w_count:\r\n",
        "                    P_w_count[token] += 1\r\n",
        "                else:\r\n",
        "                    P_w_count[token] = 1\r\n",
        "                \r\n",
        "                if token in P_w_k:\r\n",
        "                    if target in P_w_k[token]:\r\n",
        "                        P_w_k[token][target] += 1\r\n",
        "                    else:\r\n",
        "                        P_w_k[token][target] = 1\r\n",
        "                else:\r\n",
        "                    P_w_k[token] = dict()\r\n",
        "                    P_w_k[token][target] = 1\r\n",
        "                \r\n",
        "                for ntarget in P_k.keys():\r\n",
        "                    if token in P_w_nk:\r\n",
        "                        if target in P_w_nk[token]:\r\n",
        "                            P_w_nk[token][target] += 1\r\n",
        "                        else:\r\n",
        "                            P_w_nk[token][target] = 1\r\n",
        "                    else:\r\n",
        "                        P_w_nk[token] = dict()\r\n",
        "                        P_w_nk[token][target] = 1\r\n",
        "    \r\n",
        "    P_w_N = sum(P_w_count.values())\r\n",
        "    P_w = dict(map(lambda x: (x[0], x[1]/P_w_N), P_w_count.items()))\r\n",
        "\r\n",
        "    for w in P_w_k.keys():\r\n",
        "        P_w_k[w] = dict(map(lambda x: ( x[0], x[1] * P_w[w]/ (x[1] + (P_w_nk[w][x[0]] if w in P_w_nk and x[0] in P_w_nk else 0)) ), P_w_k[w].items()))\r\n",
        "\r\n",
        "    for w in P_w_nk.keys():\r\n",
        "        P_w_nk[w] = dict(map(lambda x: ( x[0], x[1] * P_w[w] / (x[1] + (P_w_k[w][x[0]] if w in P_w_k and x[0] in P_w_k[w] else 0)) ), P_w_nk[w].items()))\r\n",
        "    \r\n",
        "    with open(save_to, 'w') as fd:\r\n",
        "        json.dump({ 'P_w_k': P_w_k, 'P_w_nk': P_w_nk, 'P_k': P_k }, fd)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-q-UnPM3gun"
      },
      "source": [
        "def find_class(raw_context, h, t, model_path=\"./probabilities.json\", model=None):\r\n",
        "    if model is None:\r\n",
        "        with open(model_path, 'r') as fd:\r\n",
        "            model = json.load(fd)\r\n",
        "    P_w_k = model['P_w_k']\r\n",
        "    P_w_nk = model['P_w_nk']\r\n",
        "    P_k = model['P_k']\r\n",
        "    context = raw_context.replace(h, \"\").replace(t, \"\")\r\n",
        "    words = word_tokenize(context)\r\n",
        "    tokens = [ w.lower() for w in words if w.lower() not in stop_words ]\r\n",
        "    P_k_ws = dict()\r\n",
        "    for k in P_k.keys():\r\n",
        "        t = reduce(mul, [ P_w_k[token][k] if token in P_w_k and k in P_w_k[token] else 0 for token in tokens ], 1) * P_k[k]\r\n",
        "        b = reduce(mul, [ P_w_k[token][k] if token in P_w_k and k in P_w_k[token] else 0 for token in tokens ], 1) * P_k[k] + reduce(mul, [ P_w_nk[token][k] if token in P_w_nk and k in P_w_nk[token] else 0 for token in tokens ], 1) * (1 - P_k[k])\r\n",
        "        if b == 0:\r\n",
        "            P_k_ws[k] = 0\r\n",
        "        else:\r\n",
        "            P_k_ws[k] = t / b\r\n",
        "    return P_k_ws, max(P_k_ws, key=P_k_ws.get)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYu986V63kJ4"
      },
      "source": [
        "def test(df):\r\n",
        "    with open(\"./probabilities.json\", 'r') as fd:\r\n",
        "        model = json.load(fd)\r\n",
        "    score = 0\r\n",
        "    with tqdm(total=len(df.index)) as pbar:\r\n",
        "        for i, row in df.iterrows():\r\n",
        "            pbar.update(1)\r\n",
        "            probs, target = find_class(row[\"context\"],row[\"h\"],row[\"t\"], model=model)\r\n",
        "            if int(target) == int(row[\"target\"]):\r\n",
        "                score += 1\r\n",
        "    return score / len(df.index)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMr9iJJ93m0e"
      },
      "source": [
        "df = pd.read_csv('./dataset-annotated.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9fePADx3qu5"
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzqmZnB23taW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260fc6e0-da5b-4ecd-ff3c-325346bb37b7"
      },
      "source": [
        "# Training\r\n",
        "train(train_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9858/9858 [00:54<00:00, 180.40it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxZ9KQ7u3tM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc1702c-f46f-4b06-8a59-3f304731d821"
      },
      "source": [
        "# Testing\r\n",
        "accuracy = test(test_df)\r\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2465/2465 [00:34<00:00, 70.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.2385395537525355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}